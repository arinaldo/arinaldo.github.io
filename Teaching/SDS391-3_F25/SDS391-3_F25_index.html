<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>SDS 391-3: Linear Models</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>SDS 391-3: Linear Models</h1>
<div id="subtitle"><br /> 
</div>
</div>
<p><a href="https://arinaldo.github.io" target=&ldquo;blank&rdquo;>Alessandro (Ale) Rinaldo</a> - Fall, 2025
</p>
<p>SDS 387 is an intermediate graduate course in theoretical statistics for PhD students, covering two separate but interrelated topics: (i) stochastic convergence, (ii) selected topics in learning theory and (iii) linear regression modeling. The material and style of the course will skew towards the mathematical and theoretical aspects of common models and methods, in order to provide a foundation for those who wish to pursue research in statistical methods and theory. 
<b>This is not an applied regression analysis course.</b>
</p>
<p><br />
</p>
<div class="infoblock">
<div class="blockcontent">
<ul>
<li><p>Syllabus: <a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/SDS391-3_F25_Syllabus.pdf" target=&ldquo;blank&rdquo;>Syllabus</a>
</p>
</li>
</ul>
<ul>
<li><p>Lectures: Tuesday and Thursday, 9:00am - 10:30am, FAC 101B
</p>
</li>
</ul>
<ul>
<li><p>TA: Hien Dang, <a href="mailto:hiendang@utexas.edu" target=&ldquo;blank&rdquo;>hiendang@utexas.edu</a> - Office hours: Tuesday, 2:00–3:00 pm in WEL 5.228H
</p>
</li>
</ul>
<ul>
<li><p>Ale's Office hours: by appointment
</p>
</li>
</ul>
<ul>
<li><p>Homework submission and solutions: use <a href="https://canvas.utexas.edu/index.html" target=&ldquo;blank&rdquo;>Canvas</a>
</p>
</li>
</ul>
</div></div>
<p><br />
</p>
<table id="Homework">
<tr class="r1"><td class="c1"></td><td class="c2"> <b>Due date</b>  </td></tr>
<tr class="r2"><td class="c1">Homework 1 </td><td class="c2">   </td></tr>
<tr class="r3"><td class="c1"></td><td class="c2"> 
</td></tr></table>
<p><br />
</p>
<h2>Tuesday, August 26</h2>
<p>Class canceled (Ale was sick)
</p>
<h2>Thursday, August 28</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_01_Aug28.pdf" target=&ldquo;blank&rdquo;>Lecture 1</a>: Introduction and course logistics. Deterministic convergence and convergence with probability one. Limsup and liminf of sequences of events.<br />
</p>
<h2>Tuesday, September 2</h2>
<p>Nicola's Baritellt candidacy talk, part of which covered the content of his <a href="https://arxiv.org/pdf/2504.11360" target=&ldquo;blank&rdquo;>latest paper</a> with Stephen Walker and Bernardo Flores.<br />
</p>
<h2>Thursday, September 4</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_02_Sep4.pdf" target=&ldquo;blank&rdquo;>Lecture 2</a>: Limsup and liminf of events. Borel-Cantelli's Second Lemma. Convergence in probability and comparison with convergence with probability one. 
<br />
References:
</p>
<ul>
<li><p>See Ferguson's book, chapters 1, 2 and 4.
</p>
</li>
<li><p>A nice <a href="https://www.mynl.com/blog?id=1b77e8fad893fab23ff23e6ea696610b" target=&ldquo;blank&rdquo;>webpage</a> summarizing the different modes of stochastic convergence and providing some good examples to illustrate their differences.<br />
</p>
</li>
</ul>
<h2>Tuesday, September 9</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_03_Sep9.pdf" target=&ldquo;blank&rdquo;>Lecture 3</a>: WLLN and SLLN. Glivenko Cantelli Theorem and DKW inequality. For proof of the Glivenko-Cantelli Theorem, see Theorem 19.1 in van der Vaart's book.
</p>
<h2>Thursday, September 11</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_04_Sep11.pdf" target=&ldquo;blank&rdquo;>Lecture 4</a>: Proof of Glivenko Cantelli Theorem and DKW inequality. Lp spaces and convergence.
</p>
<h2>Tuesday, September 16</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_05_Sep16.pdf" target=&ldquo;blank&rdquo;>Lecture 5</a>: Lp convergence, Minkowski, Holder and Jensen inequalities. Relations between Lp convergence and convergence in probability and with probability one. Convergence in distribution for univariate random variables. C.d.f.'s in multivariate settings.
</p>
<h2>Thursday, September 18</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_06_Sep18.pdf" target=&ldquo;blank&rdquo;>Lecture 6</a>: 
Convergence in distribution. Relation with other forms of convergence. Marginal vs joint convergence in distribution.  For the proof of the claim that convergence in probability implies convergence in distribution, see page 330 of Billingsley's book /Probability and Measure.
</p>
<h2>Tuesday, September 23</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_07_Sep23.pdf" target=&ldquo;blank&rdquo;>Lecture 7</a>: 
Uniqueness of stochastic limits. Portmantreau Theorem (see, e.g, chapter 2 in van der Vaart's Asymptotic Statistics book).
</p>
<h2>Thursday, September 25</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_08_Sep25.pdf" target=&ldquo;blank&rdquo;>Lecture 8</a>: 
Characteristics functions and Continuity Theorem, Cramer-Wald device. I suggest reading Chapter 3 of Ferguson's book (in particuar, Theorem 3(e) has a neat proof). For a reference to multivariate Taylor series expansions, see, e.g., Advanced Calculus by G. Folland, available <a href="https://sites.math.washington.edu//~folland/AdvCalc24.pdf" target=&ldquo;blank&rdquo;>here</a>.
</p>
<h2>Tuesday, September 30</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_09_Sep30.pdf" target=&ldquo;blank&rdquo;>Lecture 9</a>: 
Slutsky's theorem, more on convergence in distribution. Big-oh and little-oh notation. Prohorov's theorem about tightness of stochastic sequences.
</p>
<h2>Thursday, October 2</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_10_Oct02.pdf" target=&ldquo;blank&rdquo;>Lecture 10</a>: 
CLT for i.i.d. variables using characteristic functions. Triangular arrays, Lindeberg Feller and Lyapunov conditions. 
</p>
<h2>Tuesday, October 7</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_11_Oct07.pdf" target=&ldquo;blank&rdquo;>Lecture 11</a>: 
Lindeberg Feller, examples and multivariate extension. Berry-Esseen bounds. A good reference for this lecture and the last is the book <i>Sums of Independent Random Variables,</i> by V.V. Petrov, Springer, 1975. Another classic and good reference is <i>Approximation Theorems of Mathematical Statistics</i> by Serfling, Wiley, 1980.
</p>
<h2>Thursday, October 9</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_12_Oct09.pdf" target=&ldquo;blank&rdquo;>Lecture 12</a>: 
High dimensional Berry-Esseen Central Limit Theorems. Some references:
</p>
<ul>
<li><p>Bentkus V. (2003). On the dependence of the Berry-Esseen bound on dimension. — J. Statist. Planning and Inference, 113(12):385-402.
</p>
</li>
<li><p>Bentkus, V. (2005). A Lyapunov-type bound in Rd. Theory of Probability &amp; Its Applications, 49(2):311–323
</p>
</li>
<li><p>Raic, M. (2019). A multivariate Berry–Esseen theorem with explicit constants. Bernoulli, 25(4A):2824–2853.
<br /><br /> 
Specific references for the class of hyper-rectangles are:
</p>
</li>
<li><p>Chernozhukov, V., Chetverikov, D. and Koike Y. (2023). Nearly optimal central limit theorem and bootstrap approximations in high dimensions, Annals of Applied Probability, 33(3): 2374-2425
</p>
</li>
<li><p>Bong, H., Kuchibhotla A. K. and Rinaldo, A. (2023). Dual Induction CLT for High-dimensional m-dependent Data, arXiv:2306.14299
<br /><br />
In class I alluded to Theorem 1 of the following paper as a very general and useful Berry-Esseen bound for smooth (thrice continuously differentiable) functions of independent random variables:
</p>
</li>
<li><p>Chatterjee, S. (2006). A generalization of the Lindeberg principle, Ann. Probab. 34 (6), 2061-2076.
</p>
</li>
</ul>
<h2>Tuesday, October 14</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_13_Oct14.pdf" target=&ldquo;blank&rdquo;>Lecture 13</a>: 
Review of linear algebra. See references in the class notes.
</p>
<h2>Thursday, October 16</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_14_Oct16.pdf" target=&ldquo;blank&rdquo;>Lecture 14</a>: 
Review of linear algebra. See references in the class notes.
</p>
<h2>Tuesday, October 21</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_15_Oct21.pdf" target=&ldquo;blank&rdquo;>Lecture 15</a>: 
Review of linear algebra. See references in the class notes. Projection of a random variable onto a vector space of random variables - see chapter 11 of Asymptotic Statistics by A. van der Vaar. 
</p>
<h2>Thursday, October 23</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_16_Oct23.pdf" target=&ldquo;blank&rdquo;>Lecture 16</a>: 
Projection of a random variable onto a vector space of random variables (cont'd). Introduction to regression modeling.
</p>
<h2>Tuesday, October 28</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_17_Oct28.pdf" target=&ldquo;blank&rdquo;>Lecture 17</a>: 
Linear regression modeling: inference vs prediction. Projection parameters and predictive risk. 
HIghly recommended reading:
Assumption lean regression. Highly recommended reading:
</p>
<ul>
<li><p>Buja, A., Brown, L., Berk, R., George, E., Pitkin, E., Traskin, M., Zhang, K., Zhao, L. (2019). Models as Approximations I: Consequences Illustrated with Linear Regression, Statistical Science, 34(4), 523-544.
</p>
</li>
</ul>
<h2>Thursday, October 30</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_18_Oct30.pdf" target=&ldquo;blank&rdquo;>Lecture 18</a>: 
Risk decomposition and pseudo inverse.
</p>
<h2>Tuesday, November 4</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_19_Nov04.pdf" target=&ldquo;blank&rdquo;>Lecture 19</a>: 
Risk decomposition and pseudo inverse.
Risk decomposition for the estimator of the linear regression parameters under linearity and fixed design. 
</p>
<h2>Thursday, November 6</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_20_Nov06.pdf" target=&ldquo;blank&rdquo;>Lecture 20</a>: 
Gauss-Markov Theorem. Ridge regression. 
</p>
<h2>Tuesday, November 11</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_21_Nov11.pdf" target=&ldquo;blank&rdquo;>Lecture 21</a>: 
Minimax lower bound for OLS. A great reference (for the random design settings) is 
</p>
<ul>
<li><p>Mourtada, J. (2022). Exact minimax risk for linear least squares, and the lower tail of sample covariance matrices. The Annals of Statistics, Vol. 50, No. 4, 2157–2178
</p>
</li>
</ul>
<h2>Thursday, November 13</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_22_Nov13.pdf" target=&ldquo;blank&rdquo;>Lecture 22</a>: 
Minimax lower bound for OLS (cont'd). Statistical inference for the linear regression parameters.
</p>
<h2>Tuesday, November 18</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_23_Nov18.pdf" target=&ldquo;blank&rdquo;>Lecture 23</a>: 
Consistency and asymptotic normality of the OLS in fixed design settings.
</p>
<h2>Thursday, November 20</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS391-3_F25/Class_Notes/Lecture_24_Nov20.pdf" target=&ldquo;blank&rdquo;>Lecture 24</a>: 
Random design. Risk formula for the OLS. Projection parameters.
</p>
</div>
</body>
</html>

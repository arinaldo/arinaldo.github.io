<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>SDS 387: Linear Models</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>SDS 387: Linear Models</h1>
<div id="subtitle"><br /> 
</div>
</div>
<p><a href="https://arinaldo.github.io" target=&ldquo;blank&rdquo;>Alessandro (Ale) Rinaldo</a> - Fall, 2024
</p>
<p>SDS 387 is an intermediate graduate course in theoretical statistics for PhD students, covering two separate but interrelated topics: (i) stochastic convergence and (ii) linear regression modeling. The material and style of the course will skew towards the mathematical and theoretical aspects of common models and methods, in order to provide a foundation for those who wish to pursue research in statistical methods and theory. 
<b>This is not an applied regression analysis course.</b>
</p>
<p><br />
</p>
<div class="infoblock">
<div class="blockcontent">
<ul>
<li><p>Syllabus: <a href="https://arinaldo.github.io/Teaching/SDS387_F24/SDS387_F24_Syllabus.pdf" target=&ldquo;blank&rdquo;>Syllabus</a>
</p>
</li>
</ul>
<ul>
<li><p>Lectures: Tuesday and Thursday, 9:00am - 10:30am, PMA 5.112
</p>
</li>
</ul>
<ul>
<li><p>TA: Khai Nguyen, <a href="mailto:khainb@utexas.edu" target=&ldquo;blank&rdquo;>khainb@utexas.edu</a> - Office hours: Thursday, 1:30pm - 2:30pm, GDC 7.418 (Poisson Bowl)
</p>
</li>
</ul>
<ul>
<li><p>Ale's Office hours: by appointment
</p>
</li>
</ul>
<ul>
<li><p>Homework submission and solutions: use <a href="https://canvas.utexas.edu/index.html" target=&ldquo;blank&rdquo;>Canvas</a>
</p>
</li>
</ul>
</div></div>
<p><br />
</p>
<table id="Homework">
<tr class="r1"><td class="c1"></td><td class="c2"> <b>Due date</b>  </td></tr>
<tr class="r2"><td class="c1">Homework 1 </td><td class="c2">  September 17  </td></tr>
<tr class="r3"><td class="c1">Homework 2 </td><td class="c2">  October 3  </td></tr>
<tr class="r4"><td class="c1">Final project proposal </td><td class="c2">  October 12  </td></tr>
<tr class="r5"><td class="c1">Homework 3 </td><td class="c2">  October 17  </td></tr>
<tr class="r6"><td class="c1">Homework 4 </td><td class="c2">  November 14  </td></tr>
<tr class="r7"><td class="c1"></td><td class="c2"> 
</td></tr></table>
<p><br />
</p>
<h2>Tuesday, August 27</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_01_Aug27.pdf" target=&ldquo;blank&rdquo;>Lecture 1</a>: Introduction and course logistics. Deterministic convergence and convergence with probability one. <br />
</p>
<h2>Thursday, August 29</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_02_Aug29.pdf" target=&ldquo;blank&rdquo;>Lecture 2</a>: Lim sup and lim inf of events. Borel Cantelli Lemmas. Convergence in probability and comparison with convergence with probability one. Law of large numbers. Glivenko Cantelli Lemma.
<br />
References:
</p>
<ul>
<li><p>See Ferguson's book, chapters 1, 2 and 4.
</p>
</li>
<li><p>For a proof of Glivenko-Cantelli's Lemma see Theorem 19.1 of van der Vaart's book.
</p>
</li>
<li><p>A nice <a href="https://www.mynl.com/blog?id=1b77e8fad893fab23ff23e6ea696610b" target=&ldquo;blank&rdquo;>webpage</a> summarizing the different modes of stochastic convergence and providing some good examples to illustrate their differences.
</p>
</li>
</ul>
<h2>Tuesday, September 3</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_03_Sep3.pdf" target=&ldquo;blank&rdquo;>Lecture 3</a>:
Glivenko Cantelli Theorem, First Borel Cantelli Lemma, more on convergence in probability.
For the Glivenko Cantelli Theorem, see Theorem 19.1 in van der Vaart's book.
</p>
<h2>Thursday, September 5</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_04_Sep5.pdf" target=&ldquo;blank&rdquo;>Lecture 4</a>: Lp convergence, Minkowski, Holder and Jensen inequalities. Relations between Lp convergence and convergence in probability and with probability one. C.d.f.'s in multivariate settings.
</p>
<h2>Tuesday, September 10</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_05_Sep10.pdf" target=&ldquo;blank&rdquo;>Lecture 5</a>: Convergence in distribution. Relation with other forms of convergence. Marginal vs joint convergence in distribution. Portmanteau theorem. For the proof of the claim that convergence in probability implies convergence in distribution, see page 330 of Billingsley's book <i>Probability and Measure.</i>
</p>
<h2>Thusday, September 12</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_06_Sep12.pdf" target=&ldquo;blank&rdquo;>Lecture 6</a>: Portmantreau Theorem, Continuous Mapping Theorem, characteristics functions and Continuity Theorem, Cramer-Wald device. I suggest reading Chapter 3 of Ferguson's book (in particuar, Theorem 3(e) has a neat proof).
</p>
<h2>Tuesday, September 17</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_07_Sep17.pdf" target=&ldquo;blank&rdquo;>Lecture 7</a>: Slutsky's theorem, more on convergence in distribution. Big-oh and little-oh notation.
</p>
<h2>Thursday, September 19</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_08_Sep19.pdf" target=&ldquo;blank&rdquo;>Lecture 8</a>: More on big-oh and little-oh notation. CLT for i.i.d. variables using characteristic functions. Triangular arrays, Lindeberg Feller and Lyapunov conditions. 
</p>
<h2>Tuesday, September 24</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_09_Sep24.pdf" target=&ldquo;blank&rdquo;>Lecture 9</a>: Lindeberg Feller, examples and multivariate extension. Berry-Esseen bounds. A good reference for this lecture and the last is the book <i>Sums of Independent Random Variables,</i> by V.V. Petrov, Springer, 1975. Another classic and good reference is <i>Approximation Theorems of Mathematical Statistics</i> by Serfling, Wiley, 1980.
</p>
<h2>Thursday, September 27</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_10_Sep27.pdf" target=&ldquo;blank&rdquo;>Lecture 10</a>: Kolmogorov Smirnov, total variation and Wasserstein distances. Theorem 1.1 about Lindeberg approximations for 3-times continuously differentiable functions. 
</p>
<h2>Tuesday, October 1</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_11_Oct1.pdf" target=&ldquo;blank&rdquo;>Lecture 11</a>: Review of linear algebra. See references in the class notes.
</p>
<h2>Thursday, October 3</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_12_Oct3.pdf" target=&ldquo;blank&rdquo;>Lecture 12</a>: Spectral properties of matrices. Eigendecomposition and singular value decomposition. 
</p>
<h2>Tuesday, October 8</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_13_Oct8.pdf" target=&ldquo;blank&rdquo;>Lecture 13</a>: Projections. Vector and matrix norms. 
</p>
<h2>Tuesday, October 15</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_14_Oct15.pdf" target=&ldquo;blank&rdquo;>Lecture 14</a>: projection of a random variable onto vector space of random variables. Introduction to linear regression modeling. For the next few lectures, I will be following closely the book <a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf" target=&ldquo;blank&rdquo;>Learning Theory from First Principles</a> by <a href="https://www.di.ens.fr/~fbach/" target=&ldquo;blank&rdquo;>Francis Bach</a>
</p>
<h2>Thursday, October 17</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_15_Oct17.pdf" target=&ldquo;blank&rdquo;>Lecture 15</a>: Inference and prediction in linear regression modeling. Projection parameter, prediction risk decomposition. 
</p>
<h2>Thursday, October 24</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_16_Oct24.pdf" target=&ldquo;blank&rdquo;>Lecture 16</a>: Geometric interpretation of the OLS estimator. Gradient descent convergence guarantee for the OLS. 
</p>
<h2>Tuesday, October 29</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_17_Oct29.pdf" target=&ldquo;blank&rdquo;>Lecture 17</a>: Pseudo inverse. Risk decomposition for the estimator of the linear regression parameters for fixed design. 
</p>
<h2>Thursday, October 31</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_18_Oct31.pdf" target=&ldquo;blank&rdquo;>Lecture 18</a>: Gauss Markov Theorem. Ridge regression. 
</p>
<h2>Tuesday, November 5</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_19_Nov5.pdf" target=&ldquo;blank&rdquo;>Lecture 19</a>: Optimal tuning for ridge regression and minimax lower bound for OLS.
</p>
<h2>Thursday, November 7</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_20_Nov7.pdf" target=&ldquo;blank&rdquo;>Lecture 20</a>: Minimax lower bound for OLS. Consistency of the OLS.
</p>
<h2>Tuesday, November 12</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_21_Nov12.pdf" target=&ldquo;blank&rdquo;>Lecture 21</a>: Asymptotic normality of the OLS estimator and statistical inference in the fixed-design, well-specified setting
</p>
<h2>Thursday, November 14</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_22_Nov14.pdf" target=&ldquo;blank&rdquo;>Lecture 22</a>: Random design. Risk formula for the OLS. Projection parameters.
</p>
<h2>Tuesday, November 19</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_23_Nov19.pdf" target=&ldquo;blank&rdquo;>Lecture 23</a>: Random design. Minimax optimality of the OLS. Exact analysis under Gaussian design. Recommended readings:
</p>
<ul>
<li><p>Exact minimax risk for linear least squares, and the lower tail of sample covariance matrices. Annals of Statistics, 50(4):2157–2178, 2022.
</p>
</li>
<li><p>Leo Breiman and David Freedman. How many variables should be entered in a regression equation? J. Amer. Statist. Assoc., 78(381):131–136, 1983.
</p>
</li>
</ul>
<h2>Thursday, November 21</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture_24_Nov21.pdf" target=&ldquo;blank&rdquo;>Lecture 24</a>: The double descend phenomenon. Recommended readings:
</p>
<ul>
<li><p>Hastie, T., Montanari, A., Rosset, S. and Tibshirani, R. J. (2022). Surprises in high-dimensional ridgeless least squares interpolation. The Annals of Statistics, 50(2), 949-986.
</p>
</li>
<li><p>Belkin, M., Hsu, D. and Xu, J. (202Two models of double descent for weak features (2020). SIAM Journal on Mathematics of Data Science, 2, 4.
</p>
</li>
</ul>
<p>Assumption lean regression. Highly recommended reading:
</p>
<ul>
<li><p>Buja, A., Brown, L., Berk, R., George, E., Pitkin, E., Traskin, M., Zhang, K., Zhao, L. (2019). Models as Approximations I: Consequences Illustrated with Linear Regression, Statistical Science, 34(4), 523-544.
</p>
</li>
</ul>
<h2>Tuesday, December 3</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture25_Dec3.pdf" target=&ldquo;blank&rdquo;>Lecture 25</a>:
Consistency and asymptotic normality of the OLS estimator in assumption lean setting. 
</p>
<h2>Thursday, December 5</h2>
<p><a href="https://arinaldo.github.io/Teaching/SDS387_F24/Lecture_Notes/Lecture26_Dec5.pdf" target=&ldquo;blank&rdquo;>Lecture 26</a>: Consistency of the plug-in estimator of the sandwich covariance for the OLS estimator. 
</p>
<p>High-dimensional generalizations of the results from the last few lectures can be found in:
</p>
<ul>
<li><p>Kuchibhotla, A., Rinaldo, A. and Wasserman, L. (2021). Berry-Esseen Bounds for Projection Parameters and Partial Correlations with Increasing Dimension, arXiv:2007.09751
</p>
</li>
<li><p>Chang, W., Kuchibhotla, A., Rinaldo, A. (2013). Inference for Projection Parameters in Linear Regression: beyond d=o(n1/2), arXiv:2307.00795.
</p>
</li>
</ul>
<p>Conditions for consistency and asymptotic normality of the OLS estimator (for a well-specified linear model) were given by
</p>
<ul>
<li><p>Lai, T. Z. and Wei, C. Z. (1982). Least Squares Estimates in Stochastic Regression Models with Applications to Identification and Control of Dynamic Systems, Annals of Statistics, 10(1): 154-166.
</p>
</li>
</ul>
<ul>
<li><p>Khamaru, K., Deshpande,  Y., and Wainwright, M. (2021). Near-optimal inference in adaptive linear regression, arXiv:2107.02266
</p>
</li>
</ul>
<p>Here is a simple example of the negative impact of adaptively collected data protocols: 
</p>
<ul>
<li><p>Shin, J., Ramdas, A., and Rinaldo A. (2021). On the Bias, Risk, and Consistency of Sample Means in Multi-armed Bandits, SIAM Journal of Mathematics of Data Science, 3(4).
</p>
</li>
</ul>
</div>
</body>
</html>

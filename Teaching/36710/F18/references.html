<h2>References</h2>


<b>Recommended Books </b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
	</table>
   <ul>
       <li> Statistics for High-Dimensional Data: Methods, Theory and
       Applications, by P. Buhlman and S. van de Geer, Springer, 2011.
       <li> Statistical Learning with Sparsity: The Lasso and Generalizations, by
       T. Hastie, R. Tibshirani and M Wainwright, Chapman & Hall, 2015.
       <li> Introduction to High-Dimensional Statistics, by C. Giraud, Chapman &
       Hall, 2015.
    <li> Concentration Inequalities: A Nonasymptotic Theory of Independencei, by S.
Boucheron, G. Lugosi and P. Massart, Oxford University Press, 2013.
<li> Rigollet, P. (2015) High-Dimensional Statistics - Lecture Notes 
<a
    href="https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2014/lecture-notes/">Lecture
    Notes</a> for the MIT
course 18.S997.
<li> High-Dimensional Probability, An Introduction with Applications in Data
Science, by R. Vershynin, 2018, available <a
    href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html">here.</a> 
    <li> Probability in High Dimension, 2016, by R. VCan Handel, 2016, available <a href="https://web.math.princeton.edu/~rvan/APC550.pdf">here.</a>
 	</ul>

	<br><br>

<b>Mon Aug 27</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>
To read more about what I referred to as the "master theorem on the asymptotics
of  parametric models" see <a href="./ch4.pdf">these notes</a> by <a
    href="https://www.stat.washington.edu/jaw/">Jon Wellner</a>. In particular,
I highly recommend looking at the notes he made for
the sequence of <a
    href="https://www.stat.washington.edu/jaw/COURSES/580s/580s.html">three
    classes on theoretical statistics</a> he has been teaching at the University
of Washington. Also, look at <a href="http://www.stat.cmu.edu/~arinaldo/Teaching/36752/S18/schedule.html">lectures of April 24 and April 26</a> of the course <a href="http://www.stat.cmu.edu/~arinaldo/Teaching/36752/S18/index.html">36-752</a>, from Spring 2018, where this "master theorem on the asymptotics
of  parametric models" is proved correctly.

<br><br>

  Parameter consistency and central limit theorems for models with increasing
  dimension d (but still d < n):            
<ul>
    <li> Rinaldo, A., G'Sell, M. and Wasserman, L. (2016+).
  Bootstrapping and Sample Splitting For High-Dimensional, Assumption-Free
  Inference, <a href="https://arxiv.org/abs/1611.05401">arxiv</a>
    <li> Wasserman, L, Kolar, M. and Rinaldo, A. (2014). Berry-Esseen bounds for
  estimating undirected graphs, Electronic Journal of Statistics, 8(1),
  1188-1224.
  <li> Fan, J. and Peng, H. (2004). Nonconcave penalized likelihood with a
      diverging number of parameters, the Annals of Statistics, 32(3),
      928-961.
    <li> Portnoy, S. (1984). Asymptotic Behavior of M-Estimators of p
  Regression, 
  Parameters when p^2/n is Large. I. Consistency, the Annals of
  Statistics, 12(4), 1298--1309.
  <li> Portnoy, S. (1985). Asymptotic Behavior of M Estimators of p Regression Parameters
      when p^2/n is Large; II. Normal Approximation, the Annals of
      Statistics, 13(4), 1403-1417.
      <li> Portnoy, S. (1988). Asymptotic Behavior of Likelihood Methods
    for Exponential Families when the Number of Parameters Tends to
    Infinity, tha Annals of Statistics, 16(1), 356-366.

   </ul>

   Some central limit theorem results in increasing dimension:

   <ul>
       <li> Chernozhukov, V.,  Chetverikov, D. and Kato, K. (2016).  Central
       Limit Theorems and Bootstrap in High Dimensions, <a
     href="http://arxiv.org/abs/1412.3661">arxiv</a>  
   <li> Bentkus, V. (2003). On the dependence of the Berry–Esseen bound on
       dimension, Journal of Statistical Planning and Inference, 113,
  385-402.
      <li> Portnoy, S. (1986). On the central limit theorem in R p when
    $p \rightarrow \infty$, Probability Theory and Related Fields,
    73(4),  571-583.
   </ul>

   <br><br>



<b>Wed Aug 31</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>

Some references to concentration inequalities:
<ul>
    <li> Concentration Inequalities: A Nonasymptotic Theory of Independence, by S.
Boucheron, G. Lugosi and P. Massart, Oxford University Press, 2013.
<li> Concentration Inequalities and Model Selection, by P. Massart, Springer Lecture
Notes in Mathematics, vol 1605, 2007.
<li> The Concentration of Measure Phenomenon, by M. Ledoux, 2005, AMS.
<li> Concentration of Measure for the Analysis of Randomized Algorithms, by D.P.
Dubhashi and A, Panconesi, Cambridge University Press, 2012.
<li>  High-Dimensional Probability, An Introduction with Applications in Data
Science, by R. Vershynin, 2018, available <a
    href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html">here.</a> 
    <li> <a href="http://www.stat.yale.edu/~pollard/Books/Mini/Basic.pdf">Chapter 2</a> of a draft monograph by David Pollard on empirrical rocesses.
</ul>

For a comprehensive treatment of sub-gaussian variables and processes (and more)
see:
<uL>
    <li> Metric Characterization of Random Variables and Random Processes, by V. V.
Buldygin, AMS, 2000.
</ul>

<br><br>



<b>Mon Sep 5</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>


    Good resources for the properties of subGaussian variables are:
    <ul>
  <li> Omar Rivasplata, Subgaussian random variables: An expository
                     note,
                     Sections
                     1, 2
                     and
                     3. <a
                     href="./subgaussians.pdf">pdf</a> 
  <li> Lecture 6 of the course "Machine learning and appplications", by Z.
      Harchaui, J. Mairal and J. Salmon,  <a
                     href="http://lear.inrialpes.fr/people/harchaoui/teaching/2013-2014/ensl/m2/lecture6.pdf">pdf</a>
     </ul>

<a href="./Hoeffding_Classic.pdf">Here</a> is the traditional bound on the mgf of a centered bounded random
variable (due to Hoeffding), implying that bounded centered variables are
sub-Guassian. It should be compared to the proof given in class. 



References for Chernoff bounds for Bernoulli (and their multiplicative forms):
<ul>
    <li> Check out the <a
  href="https://en.wikipedia.org/wiki/Chernoff_bound">Wikipedia page.</a>
    <li>    
A guided tour of chernoff bounds, by T. Hagerup and C. R\"{u}b, Information and
Processing Letters, 33(6), 305--308, 1990.
<li> Chapter 4 of the book Probability and Computing: Randomized Algorithms and
Probabilistic Analysis, by M. Mitzenmacher and E. Upfal, Cambridge University
Press, 2005.
<li> The Probabilistic Method, 3rd Edition, by N. Alon and J. H. Spencer, Wiley,
2008, Appendix A.1.
</ul>


 Improvement of Hoeffding's inequality by Berend and Kantorivich:
     <ul>
   <li> On the concentration of the missing mass, by D. Berend and A.
       Kntorovich, Electron. Commun. Probab. 18 (3), 1–7, 2013.
       <li> Section 2.2.4 in Raginski's monograph (see references at the
     top).
   </ul>

   Example of how the relative or multiplicative version of Chernoff
   bounds will lead to substantial improvements:
   <ul>
       <li>
     Minimax-optimal classification with dyadic decision trees, by
     C. Scott and R. Nowak, iEEE Transaction on Information Theory,
     52(4), 1335-1353.
   </ul>



<b>Mon Sep 17</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>

For an example of the improvement afforded by Bernstein versus Hoeffding, see
Theorem 7.1 of 
<ul>
    <li> 
 Laszlo Gyorfi, Michael Kohler, Adam Krzyzak, Harro Walk (2002). A
 Distribution-Free Theory of Nonparametric Regression, Springer. 
</ul>
available <a href="http://link.springer.com/book/10.1007%2Fb97848">here</a>.
By the way, this is an excellent book.


For sharp tail bounds for chi-squared see:
<ul>
<li> Lemma 1 in Laurent, B. and Massart, P. (2000). Adaptive estimation of a quadratic
functional by model selection, Annals of Statistics, 28(5), 1302-1338.
</ul>


For a more detailed treatment of sub-exponential variables and sharp
calculations for the corresponding tail bounds see:
<ul>
    <li> Section 2.3 and exercise 2.8 in Concentration Inequalities: A Nonasymptotic Theory of Independencei, by S.
Boucheron, G. Lugosi and P. Massart, Oxford University Press, 2013.
</ul>

For a detailed treatment of Chernoff bounds, see: 
<ul>
    <li> Section 2.3 in Concentration Inequalities and Model Selection, by P. Massart, Springer Lecture
Notes in Mathematics, vol 1605, 2007.
</ul>
<br><br>




<b>Mon Sep 24</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>

    For some refinement of the bounded difference inequality and applications,
    see:
    <ul>
  <li> Sason. I. (2011). On Refined Versions of the Azuma-Hoeffding
  Inequality with Applications in Information Theory,
  arxiv.1111.1977
    </ul>

A good referencer on U-statistics:
<ul>
  <li>
  </lu> Lee, A.J. (1990). U-Statistics: Theory and Practice, CRC Press;
</ul>

    For a comprehensive treatment of density estimation under the L1 norm see
    the book
    see:
    <ul>
  <li> Devroy, G. and Lugosi, G. (2001). Combinatorial Methods in Density
  Estimation. Springer.
</ul>

<br><br>


<br><br>

<br><br>




<b>Wed Oct 3</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>

For matrix estimation in the operator norm depending on the effective dimension,
see 
<ul>
    <li> Florentina Bunea and Luo Xiao (2015). On the sample covariance matrix
  estimator of reduced effective rank population matrices, with
  applications to fPCA, Bernoulli 21(2), 1200–1230.
</ul>


For a treatment of the matrix calculus concepts needed for proving matrix
concentration inequalities (namely operator monotone and convex matrix
functions), see:
<ul> 
    <li> R. Bhatia. Matrix Analysis. Number 169 in Graduate Texts in Mathematics. Springer, Berlin, 1997.
    <li> R. Bhatia. Positive Definite Matrices. Princeton Univ. Press, Princeton, NJ, 2007. 
</ul>

To read up about matrix concentration inequalities, I recommend:
    <ul>
  <li> Tropp, J. (2012). User-friendly tail bounds for sums of random matrices, Found. Comput. Math., Vol. 12, num. 4, pp. 389-434, 2012.
      <li> Tropp, J. (2015). An Introduction to Matrix Concentration Inequalities, Found. Trends Mach. Learning, Vol. 8, num. 1-2, pp. 1-230
    <li> Daniel Hsu, Sham M. Kakade, Tong Zhang (2011).
        Dimension-free tail inequalities for sums of random
        matrices, Electron. Commun. Probab. 17(14), 1–13. 
    </ul>


<br><br>




<b>Mon Oct 8</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>
  
    To see how Matrix Bernstein inequality can  be used in the study of random
    graphs, see Tropp's monograph and this readable reference: 
    <ul>
  <li> Fan Chung and Mary Radcliffe (2011). On the Spectra of General Random
      Graphs,  Electronic Journal of Combinatorics 18(1). 
    </ul> 
    
    To see how Matrix Bernstein inequality can be used to analyze the 
    performance of spectral clustering for the purpose of community recovery
    under a stochastic block model, see <a href="./sbpca_nips13.pdf">this old failed NIPS
  submission</a> (in
    particular, the appendix).


This is a paper on linear regression every Phd students in statistics (and everyone taking
this class) should read: 
    <ul>
  <li> Andreas Buja, Richard Berk, Lawrence Brown, Edward George, Emil
      Pitkin, Mikhail Traskin, Linda Zhao and Kai Zhang (2015). Models as
      Approximations: A Conspiracy of Random Regressors and Model
      Deviations Against Classical Inference in Regression, <a
             href="http://www-stat.wharton.upenn.edu/~buja/PAPERS/Buja_et_al_A_Conspiracy-rev1.pdf">df</a>
    </ul> 

 A nice reference on ridge and least squares regression with random covariate
    is
    <ul>
  <li> Daniel Hsu, Sham M. Kakade and Tong Zhang (2014). Random Design
      Analysis of Ridge Regression, Foundations of Computational
      Mathematics, 14(3), 569-600.  
    </ul> 



 A highly recommended book dealing extensively with the normal means problem
    is
    <ul>
  <li> Ian Johnstone, Gaussian estimation: Sequence and wavelet models
      Draft version, August 9, 2017, <a
                     href="http://statweb.stanford.edu/~imj/GE_08_09_17.pdf">pdf</a>
    </ul> 
<br><br>




<b>Mon Oct 24</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>
  
   
   
For further references on rates for the lasso, restricted eigenvalue conditions,
 oracle inequalities, etc, see
<ul>
       <li> Statistics for High-Dimensional Data: Methods, Theory and
       Applications, by P. Buhlman and S. van de Geer, Springer, 2011. Chapter 6
       and Chapter 7.
       <li>  Belloni A., Chernozhukov, D. and Hansen C. (2010)  Inference for High-Dimensional Sparse Econometric Models,
       Advances in Economics and Econometrics, ES World Congress 2010, <a
     href="https://arxiv.org/pdf/1201.0220v1.pdf">arxiv link</a>
       <li> Bickel, P. J., Y. Ritov, and A. B. Tsybakov (2009), Simultaneous
       analysis of Lasso and Dantzig selector,
      Annals of Statistics, 37(4), 1705–1732. 
   </ul>

<br><br>



<b>Wed Oct 31</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>

Good modern references on PCA:
<ul>
    <li> Johnstone, I. and Lu, A. Y. (2009) On Consistency and Sparsity for Principal Components Analysis in High Dimensions, JASA, 104(486): 682–693.
<li> B. Nadler, Finite Sample Approximation Results for principal component analysis: A matrix perturbation approach, Annals of Statistics, 36(6):2791--2817, 2008.
<li> Amini, A. and Wainwright, M. (2009). High-dimensional analysis of semidefinite relaxations for sparse principal
    components, Annals of Statistics, 37(5B), 2877-2921.
    <li> A. Birnbaum, I.M. Johnstone, B. Nadler and D. Paul, Minimax bounds for sparse PCA with noisy high-dimensional data 
the Annals of Statistics, 41(3):1055-1084, 2013.
<li> Vu, V. and Lei, J. (2013). Minimax sparse principal subspace estimation in high dimensions, Annals of
    Statistics, 41(6), 2905-2947.
</ul>
<br><br>




<b>Mon Nov 12</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>

A nice tutoerial on spectral clustering:
<ul>
    <li> A tutorial on spectral clustering, by U. von Luxburg, <a href"https://arxiv.org/abs/0711.0189">pdf</a>
</ul>
<br><br>




<b>Mon Nov 26</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>
Good references on ULLNs and classical VC theory:
    <ul>  
  <li>  Devroye, L., Gyorfi, L. and Lugosi, G. (1997). A Probabilistic Theory of
    Pattern Recognition, Springer. Chapters 12 and 13. 
    <li> Koltchinskii, V. (2011). Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery
    Problems, Springer Lecture Notes in Mathematics, 2033.
     <li> 
 Laszlo Gyorfi, Michael Kohler, Adam Krzyzak, Harro Walk (2002). A
 Distribution-Free Theory of Nonparametric Regression, Springer. Chapter 9.
</ul>
<br><br>



<b>Wed Nov 28</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>
      For relative VC deviations see:
<ul>
    <li> M. Anthony and J. Shawe-Taylor, "A result of Vapnik with applica-
    tions," Discrete Applied Mathematics, vol. 47, pp. 207-217, 1993.
    <li> V. N. Vapnik and A. Ya. Chervonenkis, "On the uniform convergence of
    rel- ative frequencies of events to their probabilities," Theory of
    Probabil- ity and its Applications, vol. 16, pp. 264-280, 1971.
   </ul>

   For Talagrand's inequality, see, e.g.,
   <ul>
    <li> Koltchinskii, V. (2011). Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery
    Problems, Springer Lecture Notes in Mathematics, 2033.
<li> The Concentration of Measure Phenomenon, by M. Ledoux, 2005, AMS.
   </ul>
<br><br>



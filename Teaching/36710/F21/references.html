<h2>References</h2>


<b>Texbooks</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
	</table>
   <ul>
    <li> Concentration Inequalities: A Nonasymptotic Theory of Independencei, by S.
Boucheron, G. Lugosi and P. Massart, Oxford University Press, 2013.
<li> High-Dimensional Probability, An Introduction with Applications in Data
Science, by R. Vershynin, Cambridge University Press, 2018, available <a
    href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html">here.</a> 
  <!--   <li> Probability in High Dimension, 2016, by R. VCan Handel, 2016, available <a href="https://web.math.princeton.edu/~rvan/APC550.pdf">here.</a>-->
 	</ul>




<br><br>

<b>Mon Aug 30</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>

A classical reference on the concentration of well-behaved functions of independent random variables is
<ul>
    <li> A new look at independence, by M. Talagrand, Annals of Probbaility,  24(1): 1-34, 1996.
    </ul>


    References for improved Chernoff bounds for Bernoulli random variables:
<ul>
    <li> Check out the <a
  href="https://en.wikipedia.org/wiki/Chernoff_bound">Wikipedia page.</a>
    <li>    
A guided tour of chernoff bounds, by T. Hagerup and C. R\"{u}b, Information and
Processing Letters, 33(6), 305--308, 1990.
<li> Chapter 4 of the book Probability and Computing: Randomized Algorithms and
Probabilistic Analysis, by M. Mitzenmacher and E. Upfal, Cambridge University
Press, 2005.
<li> The Probabilistic Method, 3rd Edition, by N. Alon and J. H. Spencer, Wiley,
2008, Appendix A.1.
</ul>


 Improvement of Hoeffding's inequality by Berend and Kantorivich:
     <ul>
   <li> On the concentration of the missing mass, by D. Berend and A.
       Kntorovich, Electron. Commun. Probab. 18 (3), 1â€“7, 2013.
       <li> Section 2.2.4 in <a href="https://www.nowpublishers.com/article/Details/CIT-064">monograph<a/> on concentration inequalities by M. Raginski.
   </ul>

   The original proof of Hoeffding inequality is <a href="./Hoeffding_Classic">here</a>. Compare to the modern, slick proof in Lemma 2.2 of [BLM].




   <br><br>

<b>Wed Sep 1</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>

For an example of the improvement afforded by Bernstein versus Hoeffding, see
<a href="./Example_Bernstein_vs_Hoeffding_Sample_Splitting.pdf">Theorem 7.1</a> of 
<ul>
    <li> 
 Laszlo Gyorfi, Michael Kohler, Adam Krzyzak, Harro Walk (2002). A
 Distribution-Free Theory of Nonparametric Regression, Springer. 
</ul>
available <a href="http://link.springer.com/book/10.1007%2Fb97848">here</a>.
By the way, this is an excellent book.


 <br><br>

<b>Wed Sep 8</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>

The book Concentration Inequalities for Sums and Martingales by B. Bercu, B. Delyon and E. Rio (2015) contains a many sharp calculations and bounds.
<br>

References on the JL Lemma:

<ul>
    <li>  Database friendly random projections: Johnson-Lindenstrauss with binary coins, by D. Achlioptas,
Journal of Computer and System Sciences 66 (2003) 671687.

<li> An Elementary Proof of a Theorem of Johnson and Lindenstrauss, by S. Dasgupta and A. Gupta, 2002.

<li> Section  1.2 of the book The Random Projection Method by S. Vempala, AMS, 2004

<li> Simple Analysis of Sparse, Sign-Consistent JL, by M. Jagadeesan, 2017, arXiv:1708.02966

<li> Optimality of the Johnson-Lindenstrauss Lemma, by K.G. Larsen and J. Nelson (2016), arXiv:1609.02094
</ul>

    

<br><br>

<b>Mon Sep 13, 15 and 20</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>



    The book Metric Characterization of Random Variables and Random Processes (2000)
V. V. Buldygin and  Yu. V. Kozachenko,  provides a great deal of information about sub-Gaussian and sub-Exponential variables (as well as random processes) and Orlicz norms.

<br><br>


To see the equivalent characterizations of sub-Gaussian and sub-exponential random variables, see Theorems 2.5.2 and 2.7.1 in Vershynin's book. In particular, Them 2.8.1. therein is yet another version of Bernstein inequality using Orlicz norm.

<br><br>

The Bernstein-Orlicz norm was introduced in the paper <a href="https://arxiv.org/abs/1111.2450">The Bernstein-Orlicz norm and deviation inequalities</a>, by S. van de Geer and J. Lederer. See also the follow up paper by J. Wellner, <a href="https://arxiv.org/abs/1703.01721">The Bennett-Orlicz norm</a>.

<br><br>

The properties of sub-Weibull random variables are described in the papers:
<ul>
<li> <a href="https://arxiv.org/abs/1804.02605">Moving Beyond Sub-Gaussianity in High-Dimensional Statistics: Applications in Covariance Estimation and Linear Regression</a>, by A.K. Kuchibhotla and A. Chakrabortty,  arXiv:1804.02605
    <li> <a href="https://arxiv.org/abs/1905.04955">Sub-Weibull distributions:
generalizing sub-Gaussian and sub-Exponential properties to heavier-tailed distributions</a>, by M. Vladimirova, S. Girard, H. Nguyen and J. Arbel arXiv:1905.04955
</ul>



<br><br>

<b>Mon Sep 29</b>
<tr>
<td colspan="2" align="center" valign="top"> <br>
<table width="80%">
<tr><td colspan="2" bgcolor="#0000FF"> <!--<img src="transparent.gif" height="1" border="0"></td></tr>-->
</tr></td><tr>
    </table>


The use of Donsker-Varadhan variational formula for the KL divergence in adaptive data analysis was introduced in the paper:
<ul>
<li> Controlling Bias in Adaptive Data Analysis Using Information Theory, by
Daniel Russo, James Zou Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, PMLR 51:1232-1240, 2016.
</ul> 


Follow-up contributions mentioned in class: 
<ul>
<li>  Information-theoretic analysis of generalization capability of learning algorithms, by Aolin Xu, Maxim Raginsky,
NIPS 2017
<li> Dependence Measures Bounding the Exploration Bias for General Measurements, by Jiantao Jiao, Yanjun Han, Tsachy Weissman, 2016, arXiv:1612.05845
</ul> 

These arguments were further developed to yield tighter generalization bounds based on chaining mutual information here:
<ul>
<li> Chaining Mutual Information and Tightening Generalization Bounds, by Amir R. Asadi1, Emmanuel Abbe1 and  Sergio Verdu', NeurIPS 2018
    </ul> 



To see the effect of data adaptivity on something as simple as the sample mean, check out
  <ul>
<li> Are sample means in multi-armed bandits positively or negatively biased? by Jaehyeok Shin, Aaditya Ramdas, Alessandro Rinaldo, NeurIPS 2018
    <li>  On the bias, risk and consistency of sample means in multi-armed bandits,  by Jaehyeok Shin, Aaditya Ramdas, Alessandro Rinaldo, 2019, arXiv:1902.00746
        <li>  On conditional versus marginal bias in multi-armed bandits, by Jaehyeok Shin, Aaditya Ramdas, Alessandro Rinaldo, 2020, ICML 2020
    </ul> 


